err <- abs(diff(oproj)) # abs and rel error
max_err[ii] <- max(pmin(err[,"abs"], err[,"rel"]))
}
max_err < tol
install.packages("usethis")
install.packages("devtools")
install.packages("roxygen2")
install.packages("testthat")
install.packages("rmarkdown")
source('~/Downloads/2020Winter/STAT 440/Q3.R')
# for each test check that `min(abs_err, rel_err) < tol`
# for each element of the solution beta_hat.
ntest <- 10 # number of tests
tol <- .01 # tolerance level
max_err <- rep(NA, ntest)
for(ii in 1:ntest) {
# simulate random data
n <- sample(100:1000, 1)
p <- sample(1:20, 1)
X <- matrix(rnorm(n*p), n, p)
beta <- rnorm(p)
y <- c(X %*% beta) + rnorm(n)
tau <- runif(1)
# fit the quantile regression
beta_hat <- qr_fit(y, X, tau)
# projection plots, except we don't plot, instead saving the output
# of each plot to an S3 object of type `optproj`.
oproj <- optim_proj(xsol = beta_hat,
fun = function(beta)
qr_obj(y = y, X = X, beta = beta, tau = tau),
maximize = FALSE, plot = FALSE)
# `diff` calculates the abs and rel error between the
# candidate solution `xsol` and the minimum in each projection plot.
# see ?diff.optcheck for details.
err <- abs(diff(oproj)) # abs and rel error
max_err[ii] <- max(pmin(err[,"abs"], err[,"rel"]))
}
max_err < tol
setwd("~/Downloads/Sample_Project_2/mbc")
devtools:install()
require(devtools)
devtools::install()
require(mbc)
#' Initial cluster allocation.
#'
#' Initializes the clusters using the kmeans++ algorithm of Arthur & Vassilvitskii (2007).
#'
#' @param y An `N x p` matrix of observations, each of which is a column.
#' @param K Number of clusters (positive integer).
#' @param max_iter The maximum number of steps in the [stats::kmeans()] algorithm.
#' @param nstart The number of random starts in the [stats::kmeans()] algorithm.
#'
#' @return A vector of length `N` consisting of integers between `1` and `K` specifying an initial clustering of the observations.
#'
#' @references Arthur, D., Vassilvitskii, S. "k-means++: the advantages of careful seeding" *Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms.* (2007): 1027â€“1035. <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>.
init_z <- function(y, K, max_iter = 10, nstart = 10) {
# init++
N <- nrow(y)
p <- ncol(y)
x <- t(y) # easier to use columns
centers <- matrix(NA, p, K) # centers
icenters <- rep(NA, K) # indices of centers
minD <- rep(Inf, N) # current minimum distance vector
# initialize
icenters[1] <- sample(N,1)
centers[,1] <- x[,icenters[1]]
for(ii in 1:(K-1)) {
D <- colSums((x - centers[,ii])^2) # new distance
minD <- pmin(minD, D)
icenters[ii+1] <- sample(N, 1, prob = minD)
centers[,ii+1] <- x[,icenters[ii+1]]
}
centers <- t(centers)
colnames(centers) <- rownames(x)
# run kmeans with these centers
km <- kmeans(x = y, centers = centers, nstart = nstart, iter.max = max_iter)
km$cluster
}
N <- 100
q <- 5
K <- 4
true_z <- sample(1:K, N, TRUE)
true_mu <- matrix(rnorm(q*K, 0, 5), ncol = q)
true_Sigma <- replicate(K, expr = {rwish(1, diag(q)*0.05, q+1)})
true_Theta <- rmNorm(N, true_mu[true_z,], true_Sigma[,,true_z])
require(mniw)
N <- 100
q <- 5
K <- 4
true_z <- sample(1:K, N, TRUE)
true_mu <- matrix(rnorm(q*K, 0, 5), ncol = q)
true_Sigma <- replicate(K, expr = {rwish(1, diag(q)*0.05, q+1)})
true_Theta <- rmNorm(N, true_mu[true_z,], true_Sigma[,,true_z])
V <- replicate(N, expr = {rwish(1, diag(q)*0.05, q+1)})
y <- rmNorm(N, true_Theta, V)
z0 <- init_z(y, K)
N_k <- freq_table(z0, K)
rho <- N_k / N
Theta <- y
mu <- t(sapply(1:K, function(k) {
if (N_k[k] > 0) {
obs_k <- y[z0==k,]
if (N_k[k] == 1) {obs_k}
else {colMeans(obs_k)}
}
else(colMeans(y))
}))
Sigma <- replicate(K, diag(q))
testing <- gibbs_sampler(z0, y, V, mu, Sigma, rho, N, q, K, returnOpt = T)
testing$mu[,,800]
true_mu
rawz <- testing$z[,800]
oldorder <- 1:K
neworder <- numeric(K)
for (i in 1:(K-1)) {
dif <- colMeans((t(true_mu)[,oldorder] - testing$mu[i,,800])^2)
imin <- which.min(dif)
neworder[i] <- oldorder[imin]
oldorder <- oldorder[-imin]
}
neworder[K] <- oldorder
newz <- neworder[testing$z[,800]]
sum(newz == true_z)
#' Solve method for variance matrices.
#'
#' @param V Variance matrix.
#' @param x Optional vector or matrix for which to solve system of equations.  If missing calculates inverse matrix.
#' @param ldV Optionally compute log determinant as well.
#'
#' @return Matrix solving system of equations and optionally the log-determinant.
#'
#' @details This function is faster and more stable than `base::solve()` when `V` is known to be positive-definite.
solveV <- function(V, x, ldV = FALSE) {
C <- chol(V) # cholesky decomposition
if(missing(x)) x <- diag(nrow(V))
# solve is O(ncol(C) * ncol(x)) with triangular matrices
# using backward subsitution
ans <- backsolve(r = C, x = backsolve(r = C, x = x, transpose = TRUE))
if(ldV) {
ldV <- 2 * sum(log(diag(C)))
ans <- list(y = ans, ldV = ldV)
}
ans
}
#' Count the number of observations within each cluster.
#'
#' @param z Cluster membership indicator vector of length N.
#' @param K Number of clusters (positive integer).
#'
#' @return A named vector of length K consisting of number of observations (`N_k`) in each cluster.
freq_table <- function(z, K) {
N_k <- sapply(1:K, function(k) sum(z == k))
names(N_k) <- 1:K
return(N_k)
}
#' @param N Number of observations (positive integer).
#'
#' @return A N x q matrix for \eqn{\Theta} drawn from its conditional posterior distribution.
#'
#' @details The conditional posterior distribution of \eqn{\Theta} is given by:
#' ```
#' \eqn{\theta_i | A - {\theta_i} \sim N(G_i(y_i - \mu_{z_i}) + \mu_{z_i}, G_i V_i)},
#' \eqn{G_i = \Sigma_{z_i} (V_i + \Sigma_{z_i})^{-1}}.
#' ```
#' Hence `Theta` is sampled from the random-effects normal distribution.
update_Theta <- function(y, V, z, mu, Sigma, N) {
rRxNorm(N, y, V, mu[z,], Sigma[,,z])
}
#'
#' @return A K x q matrix for \eqn{\mu} drawn from its conditional posterior distribution.
#'
#' @details The conditional posterior distribution of \eqn{\mu} is given by:
#' ```
#' \eqn{\mu_k | A - {\mu_1,...,\mu_K} \sim N(\bar \theta_k, Sigma_k / N_k)},
#' \eqn{\sum_{i=1}^N 1[z_i = k]}.
#' ```
#' Hence `mu` is sampled from the multivariate normal distribution.
#' Moreover, updating `mu` is allowed for only nonempty clusters (`N_k` > 0).
update_mu <- function(z, mu, Theta, Sigma, N, q, K) {
N_k <- freq_table(z, K)
nonzero <- which(N_k != 0) # find nonempty clusters
cluster_Matrix <- as.matrix(sparseMatrix(i = z, j = 1:N, x = 1))[nonzero,] # indicator matrix for observations
# average of theta for each nonempty cluster
sample_avg_Theta <- (cluster_Matrix / N_k[nonzero]) %*% Theta
# standard error of the average theta for each nonempty cluster
dnom <- array(rep(N_k[nonzero], rep(q^2, length(nonzero))),
dim = c(q,q,length(nonzero))) # make denominator (N_k) compatible with dimension of Sigma
sample_Sigma <- Sigma[,,nonzero] / dnom
# only update mu for nonempty cluster
mu[nonzero,] <- rmNorm(length(nonzero), sample_avg_Theta, sample_Sigma)
return(mu)
}
#' @param K Number of clusters (positive integer).
#' @param omega Sample covariance matrix of observed data of dimension of q x q.
#'
#' @return A q x q x K array of \eqn{\Sigma} drawn from its conditional posterior distribution.
#'
#' @details The conditional posterior distribution of \eqn{\Sigma} is given by:
#' ```
#' \eqn{\Sigma_k | A - {\Sigma_1,...,\Sigma_K} \sim InvWish(\Omega_k + 1[z_i=k] (\theta_i - \mu_k)(\theta_i - \mu_k)', N_k + v_k)}.
#' ```
#' Hence `Sigma` is sampled from the Inverse-Wishart distribution.
update_Sigma <- function(z, Theta, mu, q, K, omega) {
N_k <- freq_table(z, K)
v_k <- q + 2 # such that a priori E[Sigma_k] = var(Y)
# list of scale matrix for the Inverse-Wishart distribution
l <- lapply(1:K, function(k) {
if (N_k[k] > 0) {
Theta_mu <- Theta[z==k,] - mu[rep(k, N_k[k]),]
# special case where N_k = 1 and must use `tcrossprod` instead of `crossprod`
if (N_k[k] == 1) {cp <- tcrossprod(Theta_mu)}
else {cp <- crossprod(Theta_mu)}
}
else {cp <- 0}
return(cp + omega)
})
riwish(K, array(unlist(l), dim = c(q,q,K)), N_k + v_k)
}
#' Random sampling from the Dirichlet distribution.
#'
#' @param n Number of random draws.
#' @param alpha Weight parameter: a vector of nonnegative entries.
#' @return A matrix of size `n x length(alpha)` of which each row is a random draw.
rdirichlet <- function(n, alpha) {
K <- length(alpha) # number of categories
X <- matrix(rgamma(n*K, shape = alpha), K, n)
drop(t(sweep(X, 2, colSums(X), "/")))
}
#' @param z Cluster membership indicator vector of length N.
#' @param K Number of clusters (positive integer).
#'
#' @return A vector of length K of \eqn{\rho} drawn from its conditional posterior distribution.
#'
#' @details The conditional posterior distribution of \eqn{\rho} is given by:
#' ```
#' \eqn{\rho | A - \{ \rho \} \sim Dirichlet(\alpha)},    \eqn{\alpha = (N_1 + 1 ,..., N_K + 1)}.
#' ```
#' Hence `rho` is sampled from the Dirichlet distribution.
update_rho <- function(z, K) {
N_k <- freq_table(z, K)
alpha <- N_k + 1
rdirichlet(1, alpha)
}
#' @param K Number of clusters (positive integer).
#'
#' @return A list consisting of a vector of length N for \eqn{z}, and a N x K matrix for \eqn{lambda} (see details).
#'
#' @details The conditional posterior distribution of \eqn{z} is given by:
#' ```
#' \eqn{\kappa_{ik} = log \rho_k + log \phi(\theta_i | \mu_k, \Sigma_k) + CONST},
#' \eqn{z_i | A - \{ z \} \sim Multinomial(K, \lambda_i)},    \eqn{\lambda_{ik} = exp(\kappa_{ik}) / \sum^K_{m=1} exp(\kappa_{im}))}.
#' ```
#'
update_z <- function(Sigma, Theta, mu, rho, N, q, K) {
inSigma <- lapply(1:K, function(k) {solveV(Sigma[,,k])})
Theta_mu <- lapply(1:K, function(k) {Theta - mu[rep(k, N),]})
exp_kappa <- sapply(1:K, function(k) {rho[k] * exp(-0.5*diag(Theta_mu[[k]] %*% inSigma[[k]] %*% t(Theta_mu[[k]])))})
lambda <- exp_kappa / rowSums(exp_kappa)
new_z <- sapply(1:N, function(i) {which(rmultinom(1, 1, lambda[i,]) == 1)})
return(list(updated_z = new_z, lambda = lambda))
}
require(mniw)
require(Matrix)
N <- 100
q <- 5
K <- 4
z <- sample(1:K, N, TRUE)
mu <- matrix(rnorm(q*K, 0, 5), ncol = q)
Sigma <- replicate(K, expr = {rwish(1, diag(q), q+1)})
V <- replicate(N, expr = {rwish(1, diag(q), q+1)})
Theta <- rmNorm(N, mu[z,], Sigma[,,z])
y <- rmNorm(N, Theta, V)
Var_y <- cov(y)
v_k <- q+2
N_k <- freq_table(z, K)
rho <- update_rho(z, K)
mu <- matrix(rnorm(q*K, 0, 5), ncol = q)
Sigma <- replicate(K, diag(q))
# log likelihood function
ll <- function(y, Theta, V, Sigma, mu, z, rho, N, q, K, v_k, Var_y) {
N_k <- freq_table(z, K)
prior <- -0.5 * sum(sapply(1:K, function(k) {
S <- Sigma[,,k]
(v_k + q + 1) * log(det(S)) + sum(diag(solveV(S) %*% Var_y))
}))
first <- sum(sapply(1:N, function(i) {(y - Theta)[i,] %*% solveV(V[,,i]) %*% (y - Theta)[i,]}))
second <- sum(sapply(1:K, function(k) {
Theta_mu <- Theta[z==k,] - mu[rep(k, N_k[k]),]
S <- Sigma[,,k]
sum(diag(tcrossprod(Theta_mu %*% solveV(S), Theta_mu)) + log(det(S)))
}))
third <- sum(log(rho) * N_k)
prior + -0.5*(first + second) + third
}
testsize <- 10
testTheta <- replicate(testsize, update_Theta(y, V, z, mu, Sigma, N))
G <- lapply(1:N, function(i) {Sigma[,,z[i]] %*% solveV(V[,,i] + Sigma[,,z[i]])})
mu_Theta <- t(sapply(1:N, function(i) {t(G[[i]] %*% (y[i,] - mu[z[i],])) + mu[z[i],]}))
Sigma_Theta <- array(unlist(lapply(1:N, function(i) {G[[i]] %*% V[,,i]})), dim = c(q,q,N))
test1 <- sapply(1:testsize, function(i) {
sum(dmNorm(testTheta[,,i], mu = mu_Theta, Sigma = Sigma_Theta, log = TRUE))
})
test2 <- sapply(1:testsize, function(i) {
Theta_i <- testTheta[,,i]
ll(y, Theta_i, V, Sigma, mu, z, rho, N, q, K, v_k, Var_y)
})
test1 - test2
testsize <- 10
testmu <- replicate(testsize, update_mu(z, mu, Theta, Sigma, N, q, K))
sample_avg_Theta <- t(sapply(1:K, function(k) {colMeans(Theta[z==k,])}))
sample_Sigma <- array(unlist(lapply(1:K, function(k) {Sigma[,,k] / sum(z==k)})), dim = c(q,q,K))
test1 <- sapply(1:testsize, function(i) {
sum(dmNorm(testmu[,,i], mu = sample_avg_Theta, Sigma = sample_Sigma, log = TRUE))
})
test2 <- sapply(1:testsize, function(i) {
mu_i <- testmu[,,i]
ll(y, Theta, V, Sigma, mu_i, z, rho, N, q, K, v_k, Var_y)
})
test1 - test2
testsize <- 10
testSigma <- replicate(testsize, update_Sigma(z, Theta, mu, q, K, Var_y))
mean_Sigma <- lapply(1:K, function(k) {crossprod(Theta[z==k,] - mu[rep(k, N_k[k]),]) + Var_y})
test1 <- sapply(1:testsize, function(i) {
sum(sapply(1:K, function(k) {
diwish(testSigma[,,k,i], mean_Sigma[[k]], N_k[k] + v_k, log = TRUE)
}))
})
test2 <- sapply(1:testsize, function(i) {
Sigma_i <- testSigma[,,,i]
ll(y, Theta, V, Sigma_i, mu, z, rho, N, q, K, v_k, Var_y)
})
test1 - test2
testsize <- 10
testrho <- replicate(testsize, update_rho(z, K))
#' Density of the Dirichlet distribution.
#'
#' @param x Observation vector of nonnegative entries which sum to one.
#' @param alpha Weight parameter: a vector of nonnegative entries the same length as `x`.
#' @param log Logical; whether to evaluate the density on the log scale.
#'
#' @return The density or log-density evaluated at the inputs (scalar).
ddirichlet <- function(x, alpha, log = FALSE) {
ld <- sum(lgamma(alpha)) - lgamma(sum(alpha))
ld <- ld + sum((alpha-1) * log(x))
if(!log) ld <- exp(ld)
ld
}
test1 <- sapply(1:testsize, function(i) {
sum(ddirichlet(testrho[,i], N_k + 1, log = TRUE))
})
test2 <- sapply(1:testsize, function(i) {
rho_i <- testrho[,i]
ll(y, Theta, V, Sigma, mu, z, rho_i, N, q, K, v_k, Var_y)
})
test1 - test2
testsize <- 10
testz <- replicate(testsize, update_z(Sigma, Theta, mu, rho, N, q, K)$updated_z)
inSigma <- lapply(1:K, function(k) {solveV(Sigma[,,k])})
Theta_mu <- lapply(1:K, function(k) {Theta - mu[rep(k, N),]})
exp_kappa <- sapply(1:K, function(k) {rho[k] * exp(-0.5*diag(Theta_mu[[k]] %*% inSigma[[k]] %*% t(Theta_mu[[k]])))})
lambda <- exp_kappa / rowSums(exp_kappa)
test1 <- sapply(1:testsize, function(i) {
sum(sapply(1:N, function(j) {
log(dmultinom(as.numeric(1:K == testz[j,i]), prob = lambda[j,]))
}))
})
test2 <- sapply(1:testsize, function(i) {
z_i <- testz[,i]
ll(y, Theta, V, Sigma, mu, z_i, rho, N, q, K, v_k, Var_y)
})
test1 - test2
#' Initial cluster allocation.
#'
#' Initializes the clusters using the kmeans++ algorithm of Arthur & Vassilvitskii (2007).
#'
#' @param y An `N x p` matrix of observations, each of which is a column.
#' @param K Number of clusters (positive integer).
#' @param max_iter The maximum number of steps in the [stats::kmeans()] algorithm.
#' @param nstart The number of random starts in the [stats::kmeans()] algorithm.
#'
#' @return A vector of length `N` consisting of integers between `1` and `K` specifying an initial clustering of the observations.
#'
#' @references Arthur, D., Vassilvitskii, S. "k-means++: the advantages of careful seeding" *Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms.* (2007): 1027â€“1035. <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>.
init_z <- function(y, K, max_iter = 10, nstart = 10) {
# init++
N <- nrow(y)
p <- ncol(y)
x <- t(y) # easier to use columns
centers <- matrix(NA, p, K) # centers
icenters <- rep(NA, K) # indices of centers
minD <- rep(Inf, N) # current minimum distance vector
# initialize
icenters[1] <- sample(N,1)
centers[,1] <- x[,icenters[1]]
for(ii in 1:(K-1)) {
D <- colSums((x - centers[,ii])^2) # new distance
minD <- pmin(minD, D)
icenters[ii+1] <- sample(N, 1, prob = minD)
centers[,ii+1] <- x[,icenters[ii+1]]
}
centers <- t(centers)
colnames(centers) <- rownames(x)
# run kmeans with these centers
km <- kmeans(x = y, centers = centers, nstart = nstart, iter.max = max_iter)
km$cluster
}
N <- 100
q <- 5
K <- 4
true_z <- sample(1:K, N, TRUE)
true_mu <- matrix(rnorm(q*K, 0, 5), ncol = q)
true_Sigma <- replicate(K, expr = {rwish(1, diag(q)*0.05, q+1)})
true_Theta <- rmNorm(N, true_mu[true_z,], true_Sigma[,,true_z])
V <- replicate(N, expr = {rwish(1, diag(q)*0.05, q+1)})
y <- rmNorm(N, true_Theta, V)
z0 <- init_z(y, K)
N_k <- freq_table(z0, K)
rho <- N_k / N
Theta <- y
mu <- t(sapply(1:K, function(k) {
if (N_k[k] > 0) {
obs_k <- y[z0==k,]
if (N_k[k] == 1) {obs_k}
else {colMeans(obs_k)}
}
else(colMeans(y))
}))
Sigma <- replicate(K, diag(q))
testing <- gibbs_sampler(z0, y, V, mu, Sigma, rho, N, q, K, returnOpt = T)
testing$mu[,,800]
true_mu
rawz <- testing$z[,800]
oldorder <- 1:K
neworder <- numeric(K)
for (i in 1:(K-1)) {
dif <- colMeans((t(true_mu)[,oldorder] - testing$mu[i,,800])^2)
imin <- which.min(dif)
neworder[i] <- oldorder[imin]
oldorder <- oldorder[-imin]
}
neworder[K] <- oldorder
newz <- neworder[testing$z[,800]]
sum(newz == true_z)
gibbs_sampler <- function(z0, y, V, mu, Sigma, rho, N, q, K, iter = 1000, burn = 200, returnOpt = FALSE) {
actual_iter = iter - burn
# return values
rho_matrix <- matrix(NA, nrow = K, ncol = actual_iter)
mu_matrix <- array(matrix(NA, nrow = K, ncol = q), dim = c(K,q,actual_iter))
Sigma_matrix <- array(matrix(NA, nrow = q, ncol = q), dim = c(q,q,K,actual_iter))
if (returnOpt) {
Theta_matrix <- array(matrix(NA, nrow = N, ncol = q), dim = c(N,q,actual_iter))
z_matrix <- matrix(NA, nrow = N, ncol = actual_iter)
}
Lambda_matrix <- matrix(0, nrow = N, ncol = K)
z <- z0
prev_mu <- matrix(rnorm(q*K), ncol = q) #initial mu
omega <- cov(y)
for (i in 1:iter) {
Theta <- update_Theta(y, V, z, mu, Sigma, N)
mu <- update_mu(z, prev_mu, Theta, Sigma, N, q, K)
prev_mu <- mu
Sigma <- update_Sigma(z, Theta, mu, q, K, omega)
rho <- update_rho(z, K)
mylst <- update_z(Sigma, Theta, mu, rho, N, q, K)
z <- mylst$updated_z
if (i > burn) {
idx <- i - burn
rho_matrix[,idx] <- rho
mu_matrix[,,idx] <- mu
Sigma_matrix[,,,idx] <- Sigma
if (returnOpt) {
Theta_matrix[,,idx] <- Theta
z_matrix[,idx] <- z
}
Lambda_matrix <- Lambda_matrix + mylst$Lambda
}
}
Lambda_matrix <- Lambda_matrix / matrix(rep(actual_iter, N*K), nrow = N, ncol = K)
if (returnOpt) {
return(list(rho=rho_matrix,mu=mu_matrix,Sigma=Sigma_matrix,Theta=Theta_matrix,z=z_matrix,Lambda=Lambda_matrix))
}
return(list(rho=rho_matrix,mu=mu_matrix,Sigma=Sigma_matrix,Lambda=Lambda_matrix))
}
testing <- gibbs_sampler(z0, y, V, mu, Sigma, rho, N, q, K, returnOpt = T)
testing$mu[,,800]
true_mu
rawz <- testing$z[,800]
oldorder <- 1:K
neworder <- numeric(K)
for (i in 1:(K-1)) {
dif <- colMeans((t(true_mu)[,oldorder] - testing$mu[i,,800])^2)
imin <- which.min(dif)
neworder[i] <- oldorder[imin]
oldorder <- oldorder[-imin]
}
neworder[K] <- oldorder
newz <- neworder[testing$z[,800]]
sum(newz == true_z)
setwd("~/Downloads/Sample_Project_2/mbc/R")
devtools::document()
devtools::install()
require(mbc)
devtools::install()
library(mbc)
devtools::document()
devtools::install()
require(mbc)
?remove.packages()
remove.packages()
remove.packages("mbc")
library(mbc)
devtools::install()
